{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a812712",
   "metadata": {},
   "source": [
    "# Case study 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e23f24e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guang\\AppData\\Local\\Temp\\ipykernel_14608\\478144586.py:8: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Guang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Guang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:75% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from IPython.core.display import display, HTML\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "# Increase the width of the notebook for displaying DataFrames\n",
    "display(HTML(\"<style>.container { width:75% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3bd8ac72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>Area</th>\n",
       "      <th>Classification</th>\n",
       "      <th>SubClassification</th>\n",
       "      <th>Requirement</th>\n",
       "      <th>FullDescription</th>\n",
       "      <th>LowestSalary</th>\n",
       "      <th>HighestSalary</th>\n",
       "      <th>JobType</th>\n",
       "      <th>AverageSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37404348</td>\n",
       "      <td>Casual Stock Replenisher</td>\n",
       "      <td>Aldi Stores</td>\n",
       "      <td>2018-10-07</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>North West &amp; Hills District</td>\n",
       "      <td>Retail &amp; Consumer Products</td>\n",
       "      <td>Retail Assistants</td>\n",
       "      <td>Our Casual Stock Replenishers pride themselves...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37404337</td>\n",
       "      <td>Casual Stock Replenisher</td>\n",
       "      <td>Aldi Stores</td>\n",
       "      <td>2018-10-07</td>\n",
       "      <td>Richmond &amp; Hawkesbury</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Retail &amp; Consumer Products</td>\n",
       "      <td>Retail Assistants</td>\n",
       "      <td>Our Casual Stock Replenishers pride themselves...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37404356</td>\n",
       "      <td>RETAIL SALES SUPERSTARS and STYLISTS Wanted - ...</td>\n",
       "      <td>LB Creative Pty Ltd</td>\n",
       "      <td>2018-10-07</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>CBD &amp; Inner Suburbs</td>\n",
       "      <td>Retail &amp; Consumer Products</td>\n",
       "      <td>Retail Assistants</td>\n",
       "      <td>BRAND NEW FLAGSHIP STORE OPENING - SUNSHINE PLAZA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37404330</td>\n",
       "      <td>Team member - Belrose</td>\n",
       "      <td>Anaconda Group Pty Ltd</td>\n",
       "      <td>2018-10-07</td>\n",
       "      <td>Gosford &amp; Central Coast</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Retail &amp; Consumer Products</td>\n",
       "      <td>Retail Assistants</td>\n",
       "      <td>Bring it on - do you love the great outdoors a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37404308</td>\n",
       "      <td>Business Banking Contact Centre Specialist, Ni...</td>\n",
       "      <td>Commonwealth Bank - Business &amp; Private Banking</td>\n",
       "      <td>2018-10-07</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Ryde &amp; Macquarie Park</td>\n",
       "      <td>Call Centre &amp; Customer Service</td>\n",
       "      <td>Sales - Inbound</td>\n",
       "      <td>We are seeking highly articulate, enthusiastic...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id                                              Title   \n",
       "0  37404348                           Casual Stock Replenisher  \\\n",
       "1  37404337                           Casual Stock Replenisher   \n",
       "2  37404356  RETAIL SALES SUPERSTARS and STYLISTS Wanted - ...   \n",
       "3  37404330                              Team member - Belrose   \n",
       "4  37404308  Business Banking Contact Centre Specialist, Ni...   \n",
       "\n",
       "                                          Company        Date   \n",
       "0                                     Aldi Stores  2018-10-07  \\\n",
       "1                                     Aldi Stores  2018-10-07   \n",
       "2                             LB Creative Pty Ltd  2018-10-07   \n",
       "3                          Anaconda Group Pty Ltd  2018-10-07   \n",
       "4  Commonwealth Bank - Business & Private Banking  2018-10-07   \n",
       "\n",
       "                  Location                         Area   \n",
       "0                   Sydney  North West & Hills District  \\\n",
       "1    Richmond & Hawkesbury                          NaN   \n",
       "2                 Brisbane          CBD & Inner Suburbs   \n",
       "3  Gosford & Central Coast                          NaN   \n",
       "4                   Sydney        Ryde & Macquarie Park   \n",
       "\n",
       "                   Classification  SubClassification   \n",
       "0      Retail & Consumer Products  Retail Assistants  \\\n",
       "1      Retail & Consumer Products  Retail Assistants   \n",
       "2      Retail & Consumer Products  Retail Assistants   \n",
       "3      Retail & Consumer Products  Retail Assistants   \n",
       "4  Call Centre & Customer Service    Sales - Inbound   \n",
       "\n",
       "                                         Requirement FullDescription   \n",
       "0  Our Casual Stock Replenishers pride themselves...             NaN  \\\n",
       "1  Our Casual Stock Replenishers pride themselves...             NaN   \n",
       "2  BRAND NEW FLAGSHIP STORE OPENING - SUNSHINE PLAZA             NaN   \n",
       "3  Bring it on - do you love the great outdoors a...             NaN   \n",
       "4  We are seeking highly articulate, enthusiastic...             NaN   \n",
       "\n",
       "   LowestSalary  HighestSalary JobType  AverageSalary  \n",
       "0             0             30     NaN           15.0  \n",
       "1             0             30     NaN           15.0  \n",
       "2             0             30     NaN           15.0  \n",
       "3             0             30     NaN           15.0  \n",
       "4             0             30     NaN           15.0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"preprocessed_data.csv\",low_memory=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "83ca2433",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guang\\AppData\\Local\\Temp\\ipykernel_14608\\2101394758.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['combined_text'] = filtered_data['Requirement'] + ' ' + filtered_data['FullDescription']\n",
      "C:\\Users\\Guang\\AppData\\Local\\Temp\\ipykernel_14608\\2101394758.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['processed_text'] = filtered_data['combined_text'].apply(lambda x: preprocess_text(str(x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for Topic #1:\n",
      "['management', 'team', 'role', 'business', 'skill', 'project', 'experience', 'strong', 'nbsp', 'br']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #2:\n",
      "['learning', 'nbsp', 'data', 'technology', 'work', 'people', 'experience', 'strong', 'team', 'li']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #3:\n",
      "['role', 'solution', 'service', 'work', 'business', 'skill', 'experience', 'li', 'strong', 'nbsp']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #4:\n",
      "['window', 'role', 'server', 'ul', 'service', 'network', 'experience', 'support', 'strong', 'li']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #5:\n",
      "['ul', 'process', 'technical', 'design', 'strong', 'requirement', 'experience', 'solution', 'business', 'li']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #6:\n",
      "['tool', 'ul', 'quality', 'integration', 'automation', 'strong', 'experience', 'testing', 'li', 'test']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #7:\n",
      "['day', 'note', 'recruitment', 'send', 'today', 'resume', 'water', 'apply', 'button', 'clicking']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #8:\n",
      "['role', 'credit', 'institution', 'strong', 'transport', 'investment', 'payment', 'bank', 'banking', 'financial']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #9:\n",
      "['experience', 'client', 'canberra', 'federal', 'clearance', 'australian', 'government', 'nbsp', 'li', 'strong']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #10:\n",
      "['year', 'role', 'application', 'business', 'br', 'experience', 'strong', 'client', 'em', 'recruitment']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #11:\n",
      "['intelligence', 'sql', 'report', 'experience', 'microsoft', 'erp', 'oracle', 'reporting', 'bi', 'business']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #12:\n",
      "['account', 'ul', 'new', 'business', 'marketing', 'nan', 'customer', 'sale', 'product', 'li']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #13:\n",
      "['technical', 'used', 'role', 'operation', 'maintenance', 'center', 'management', 'mining', 'information', 'asset']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #14:\n",
      "['technology', 'information', 'support', 'nbsp', 'team', 'http', 'application', 'service', 'strong', 'justify']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #15:\n",
      "['opportunity', 'team', 'dimension', 'data', 'client', 'center', 'li', 'nbsp', 'strong', 'br']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #16:\n",
      "['team', 'web', 'application', 'software', 'developer', 'ul', 'development', 'strong', 'experience', 'li']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #17:\n",
      "['etl', 'migration', 'analysis', 'ul', 'database', 'experience', 'sql', 'analytics', 'li', 'data']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #18:\n",
      "['true', 'opportunity', 'work', 'role', 'ul', 'experience', 'div', 'br', 'strong', 'li']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #19:\n",
      "['management', 'technology', 'experience', 'ul', 'information', 'risk', 'strong', 'cyber', 'li', 'security']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #20:\n",
      "['experience', 'work', 'role', 'skill', 'support', 'team', 'strong', 'customer', 'service', 'li']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #21:\n",
      "['benefit', 'employee', 'team', 'australia', 'people', 'health', 'work', 'nbsp', 'strong', 'li']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #22:\n",
      "['true', 'project', 'new', 'li', 'month', 'role', 'experience', 'contract', 'strong', 'br']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #23:\n",
      "['software', 'business', 'amp', 'ul', 'team', 'technical', 'strong', 'client', 'solution', 'li']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #24:\n",
      "['working', 'work', 'product', 'development', 'experience', 'nbsp', 'team', 'li', 'strong', 'br']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #25:\n",
      "['enterprise', 'lead', 'strategic', 'architecture', 'business', 'strong', 'technology', 'li', 'strategy', 'digital']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #26:\n",
      "['program', 'strong', 'delivery', 'stakeholder', 'experience', 'project', 'management', 'business', 'change', 'li']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #27:\n",
      "['opportunity', 'requirement', 'agile', 'process', 'analyst', 'sap', 'experience', 'business', 'strong', 'li']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #28:\n",
      "['professional', 'opportunity', 'committed', 'em', 'information', 'service', 'experience', 'li', 'business', 'strong']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #29:\n",
      "['delivery', 'program', 'manage', 'ul', 'experience', 'manager', 'strong', 'management', 'project', 'li']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #30:\n",
      "['infrastructure', 'platform', 'azure', 'aws', 'solution', 'design', 'strong', 'experience', 'cloud', 'li']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the data for computer science and IT-related jobs\n",
    "filtered_data = data[data['Classification'].str.contains('Information', na=False)]\n",
    "\n",
    "# Combine the 'Requirement' and 'FullDescription' columns\n",
    "filtered_data['combined_text'] = filtered_data['Requirement'] + ' ' + filtered_data['FullDescription']\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    # Tokenize and convert to lowercase\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    words = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "filtered_data['processed_text'] = filtered_data['combined_text'].apply(lambda x: preprocess_text(str(x)))\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = CountVectorizer(stop_words='english', max_df=0.95, min_df=2)\n",
    "text_matrix = vectorizer.fit_transform(filtered_data['processed_text'])\n",
    "\n",
    "# Apply topic modeling using LDA\n",
    "lda = LatentDirichletAllocation(n_components=30, random_state=42)\n",
    "lda.fit(text_matrix)\n",
    "\n",
    "# Print the top 10 words for each topic\n",
    "for index, topic in enumerate(lda.components_):\n",
    "    print(f'Top 10 words for Topic #{index+1}:')\n",
    "    print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f78de1d",
   "metadata": {},
   "source": [
    "My codes \n",
    "1. Filters Data for IT-related jobs\n",
    "2. Preprocesses the text data(The process function tokenizes the input text, removes non-alphabetic characters, removes stop words using the NLTK library, and lemmatizes the words using the WordNetLemmatizer from NLTK. )\n",
    "3. Applies topic modeling using Latent Dirichlet Allocation (LDA) to identify the top skills and subjects mentioned in the job listings.\n",
    "\n",
    "The above result shows that the most important skill from job market is that:\"cyber security\", \"web\", \"sql\", \"ios\", \"machine learning\" and \"programing language\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1b91ed",
   "metadata": {},
   "source": [
    "# Case study 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8debab3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Guang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Guang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "294c5069",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"preprocessed_data.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "add2dd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the 'Requirement' and 'FullDescription' columns\n",
    "data['combined_text'] = data['Requirement'] + ' ' + data['FullDescription']\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    # Tokenize and convert to lowercase\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    words = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "data['processed_text'] = data['combined_text'].apply(lambda x: preprocess_text(str(x)))\n",
    "\n",
    "# Edit Candidate's profile here\n",
    "candidate_profile = '''\n",
    "Mathew is a student in computer science. He has skills like machine learning and cyber security\n",
    "'''\n",
    "\n",
    "# Preprocess the candidate's profile\n",
    "processed_candidate_profile = preprocess_text(candidate_profile)\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "text_matrix = vectorizer.fit_transform(data['processed_text'])\n",
    "candidate_vector = vectorizer.transform([processed_candidate_profile])\n",
    "\n",
    "# Compute cosine similarity\n",
    "similarity_scores = cosine_similarity(candidate_vector, text_matrix)\n",
    "\n",
    "# Get the top 10 job indices and their similarity scores\n",
    "top_10_job_indices = np.argsort(similarity_scores[0])[-10:][::-1]\n",
    "top_10_job_scores = np.sort(similarity_scores[0])[-10:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0616b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 job recommendations for the candidate:\n",
      "1. Job ID: 37408864, Similarity Score: 0.42134648229395555\n",
      "2. Job ID: 37455360, Similarity Score: 0.4207103915603747\n",
      "3. Job ID: 37490442, Similarity Score: 0.41001996845535865\n",
      "4. Job ID: 37597983, Similarity Score: 0.4093404192805201\n",
      "5. Job ID: 37997807, Similarity Score: 0.40167285400696134\n",
      "6. Job ID: 37554941, Similarity Score: 0.3814574296069183\n",
      "7. Job ID: 37515146, Similarity Score: 0.3813083888849317\n",
      "8. Job ID: 37994290, Similarity Score: 0.38122283387662564\n",
      "9. Job ID: 37954799, Similarity Score: 0.38122283387662564\n",
      "10. Job ID: 38064961, Similarity Score: 0.3783559598060111\n"
     ]
    }
   ],
   "source": [
    "# Print the top 10 job recommendations\n",
    "print(\"Top 10 job recommendations for the candidate:\")\n",
    "for i, job_index in enumerate(top_10_job_indices):\n",
    "    print(f\"{i+1}. Job ID: {data.loc[job_index, 'Id']}, Similarity Score: {top_10_job_scores[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959e5f57",
   "metadata": {},
   "source": [
    "1. Preprocess the job market dataset and the candidate's profile (The process function tokenizes the input text, removes non-alphabetic characters, removes stop words using the NLTK library, and lemmatizes the words using the WordNetLemmatizer from NLTK. ).\n",
    "2. Vectorize the text data using TF-IDF, which converts the text into a numerical representation.\n",
    "3. Compute the cosine similarity between the candidate's profile vector and each job listing vector.\n",
    "4. Sort the job listings based on their similarity scores and recommend the top 10 jobs with the highest similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722d0f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
